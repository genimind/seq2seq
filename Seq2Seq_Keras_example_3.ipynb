{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Example-3 based on Keras tutorial on Seq2Seq [blog](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html).\n",
    "\n",
    "[dataset source (english-french)](http://www.manythings.org/anki/fra-eng.zip)\n",
    "\n",
    "In this example we'll use words as tokens with Encoder/Decoder model (different than the one use in example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, LSTM, Dense, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    x_tokenized = tokenizer.texts_to_sequences(x)\n",
    "    return x_tokenized, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    x_padded = pad_sequences(x, maxlen=length, padding='post')\n",
    "    return x_padded\n",
    "\n",
    "def unpad(x):\n",
    "    \"\"\"\n",
    "    remove the <PAD> added when padding text\n",
    "    \"\"\"\n",
    "    x = x.replace('<PAD>','')\n",
    "    return x.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sequence_to_text(sequence, tokenizer):\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    return ' '.join([index_to_words[token] for token in sequence if token > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  10000\n",
      "number of input  tokens: 3373\n",
      "number of output tokens: 5829\n",
      "Max sequence length for inputs: 8\n",
      "Max sequence length for outputs: 12\n"
     ]
    }
   ],
   "source": [
    "filename = 'fra.txt'\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "lines = open(filename).read().split('\\n')\n",
    "num_samples = 20000\n",
    "\n",
    "input_chars = set()\n",
    "target_chars = set()\n",
    "\n",
    "# process the lines\n",
    "for line in lines[:min(num_samples, len(lines)-1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "preproc_input_texts, input_texts_tk = tokenize(input_texts)\n",
    "preproc_target_texts, target_texts_tk = tokenize(target_texts)\n",
    "\n",
    "preproc_input_texts = pad(preproc_input_texts)\n",
    "preproc_target_texts = pad(preproc_target_texts)\n",
    "\n",
    "num_encoder_tokens = len(input_texts_tk.word_index) + 1\n",
    "num_decoder_tokens = len(target_texts_tk.word_index) + 1\n",
    "max_encoder_seq_length = max([len(txt) for txt in preproc_input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in preproc_target_texts])\n",
    "\n",
    "\n",
    "print ('number of samples: ', len(input_texts))\n",
    "print ('number of input  tokens:', num_encoder_tokens)\n",
    "print ('number of output tokens:', num_decoder_tokens)\n",
    "print ('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print ('Max sequence length for outputs:', max_decoder_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "preproc_target_texts = preproc_target_texts.reshape(*preproc_target_texts.shape, 1)\n",
    "#print(preproc_target_texts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# model setup using GRU\n",
    "from keras.layers import RepeatVector, Dropout, Bidirectional\n",
    "\n",
    "latent_dim = 32  # Latent dimensionality of the encoding space.\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "def encdec_model(input_shape, target_sequence_length, input_vocab_size, target_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on input and target\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param input_vocab_size: Number of unique words in the input dataset\n",
    "    :param target_vocab_size: Number of unique words in the target dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "\n",
    "    _input = Input(shape=input_shape[1:])\n",
    "#     print('_input:', _input.shape)\n",
    "    _embedded_input = Embedding(input_dim=input_vocab_size, \n",
    "                             input_length=max_encoder_seq_length, output_dim=512)(_input)\n",
    "\n",
    "    _encoded = Bidirectional(GRU(latent_dim))(_embedded_input)\n",
    "    \n",
    "    _decoded = RepeatVector(target_sequence_length)(_encoded)\n",
    "    _decoded = Bidirectional(GRU(latent_dim, return_sequences=True))(_decoded)\n",
    "    _output = Dense(256, activation='relu')(_decoded)\n",
    "    _output = Dense(target_vocab_size, activation='softmax')(_output)\n",
    "    \n",
    "    model = Model(inputs=_input, outputs=_output)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 8)\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# model.summary()\n",
    "# print(encoder_input_data.shape)\n",
    "# print(encoder_input_data.shape[-1])\n",
    "# model_as_json = json.loads(model.to_json())\n",
    "# print(json.dumps(model_as_json, indent=2))\n",
    "print(preproc_input_texts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped input:  (10000, 8)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 8, 512)            1726976   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 64)                104640    \n",
      "_________________________________________________________________\n",
      "repeat_vector_12 (RepeatVect (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 12, 64)            18624     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 12, 256)           16640     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 12, 5829)          1498053   \n",
      "=================================================================\n",
      "Total params: 3,364,933\n",
      "Trainable params: 3,364,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "7168/8000 [=========================>....] - ETA: 7s - loss: 6.2551 - acc: 0.4438 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 00000: val_loss improved from inf to 4.28313, saving model to seq2seq_weights_best_3.hdf5\n",
      "8000/8000 [==============================] - 75s - loss: 6.0266 - acc: 0.4513 - val_loss: 4.2831 - val_acc: 0.5279\n",
      "Epoch 2/10\n",
      "7168/8000 [=========================>....] - ETA: 6s - loss: 3.7965 - acc: 0.5166 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 00001: val_loss improved from 4.28313 to 3.82646, saving model to seq2seq_weights_best_3.hdf5\n",
      "8000/8000 [==============================] - 67s - loss: 3.7791 - acc: 0.5166 - val_loss: 3.8265 - val_acc: 0.5279\n",
      "Epoch 3/10\n",
      "7168/8000 [=========================>....] - ETA: 6s - loss: 3.4596 - acc: 0.5205 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 00002: val_loss improved from 3.82646 to 3.62658, saving model to seq2seq_weights_best_3.hdf5\n",
      "8000/8000 [==============================] - 67s - loss: 3.4520 - acc: 0.5224 - val_loss: 3.6266 - val_acc: 0.5281\n",
      "Epoch 4/10\n",
      "7168/8000 [=========================>....] - ETA: 6s - loss: 3.2726 - acc: 0.5434 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 00003: val_loss improved from 3.62658 to 3.56756, saving model to seq2seq_weights_best_3.hdf5\n",
      "8000/8000 [==============================] - 65s - loss: 3.2699 - acc: 0.5431 - val_loss: 3.5676 - val_acc: 0.5292\n",
      "Epoch 5/10\n",
      "7168/8000 [=========================>....] - ETA: 6s - loss: 3.1621 - acc: 0.5430 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 00004: val_loss improved from 3.56756 to 3.55072, saving model to seq2seq_weights_best_3.hdf5\n",
      "8000/8000 [==============================] - 69s - loss: 3.1583 - acc: 0.5436 - val_loss: 3.5507 - val_acc: 0.5311\n",
      "Epoch 6/10\n",
      "7168/8000 [=========================>....] - ETA: 6s - loss: 3.0913 - acc: 0.5488 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 00005: val_loss improved from 3.55072 to 3.54761, saving model to seq2seq_weights_best_3.hdf5\n",
      "8000/8000 [==============================] - 67s - loss: 3.0905 - acc: 0.5488 - val_loss: 3.5476 - val_acc: 0.5328\n",
      "Epoch 7/10\n",
      "7168/8000 [=========================>....] - ETA: 6s - loss: 3.0556 - acc: 0.5489 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 00006: val_loss did not improve\n",
      "8000/8000 [==============================] - 68s - loss: 3.0531 - acc: 0.5494 - val_loss: 3.5660 - val_acc: 0.5320\n",
      "Epoch 8/10\n",
      "7168/8000 [=========================>....] - ETA: 6s - loss: 3.0305 - acc: 0.5511 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 00007: val_loss improved from 3.54761 to 3.53340, saving model to seq2seq_weights_best_3.hdf5\n",
      "8000/8000 [==============================] - 69s - loss: 3.0272 - acc: 0.5518 - val_loss: 3.5334 - val_acc: 0.5320\n",
      "Epoch 9/10\n",
      "7168/8000 [=========================>....] - ETA: 7s - loss: 2.9832 - acc: 0.5532 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 00008: val_loss improved from 3.53340 to 3.52617, saving model to seq2seq_weights_best_3.hdf5\n",
      "8000/8000 [==============================] - 71s - loss: 2.9860 - acc: 0.5529 - val_loss: 3.5262 - val_acc: 0.5334\n",
      "Epoch 10/10\n",
      "7168/8000 [=========================>....] - ETA: 6s - loss: 2.9507 - acc: 0.5560 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 00009: val_loss improved from 3.52617 to 3.51697, saving model to seq2seq_weights_best_3.hdf5\n",
      "8000/8000 [==============================] - 70s - loss: 2.9543 - acc: 0.5551 - val_loss: 3.5170 - val_acc: 0.5337\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "batch_size = 1024  # Batch size for training.\n",
    "epochs = 10  # Number of epochs to train for.\n",
    "\n",
    "# Reshape the input\n",
    "# tmp_input = preproc_input_texts.reshape((-1, preproc_input_texts.shape[1], 1))\n",
    "tmp_input = preproc_input_texts\n",
    "\n",
    "print('reshaped input: ', tmp_input.shape)\n",
    "\n",
    "# Train the neural network\n",
    "model = encdec_model(\n",
    "    tmp_input.shape,\n",
    "    preproc_target_texts.shape[1],\n",
    "    num_encoder_tokens,\n",
    "    num_decoder_tokens)\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='seq2seq_weights_best_3.hdf5', \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model.fit(tmp_input, preproc_target_texts, \n",
    "                     batch_size=batch_size, epochs=epochs, validation_split=0.2, callbacks=[checkpointer])\n",
    "\n",
    "# save the model\n",
    "model.save('seq2seq_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x15fd60fd0>,\n",
       " <keras.layers.embeddings.Embedding at 0x15fd60f60>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x13c8cabe0>,\n",
       " <keras.layers.core.RepeatVector at 0x15fd65630>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x15f8a4b00>,\n",
       " <keras.layers.core.Dense at 0x15fd659e8>,\n",
       " <keras.layers.core.Dense at 0x15fd65b38>]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# del model\n",
    "model = load_model('seq2seq_3.h5')\n",
    "\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++--------------++\n",
      "input seq  :  I can ski.\n",
      "decoded seq:  je me le\n",
      "++--------------++\n",
      "input seq  :  I cringed.\n",
      "decoded seq:  je me un\n",
      "++--------------++\n",
      "input seq  :  I cringed.\n",
      "decoded seq:  je me un\n",
      "++--------------++\n",
      "input seq  :  I cringed.\n",
      "decoded seq:  je me un\n",
      "++--------------++\n",
      "input seq  :  I give up.\n",
      "decoded seq:  j'ai été\n",
      "++--------------++\n",
      "input seq  :  I got hot.\n",
      "decoded seq:  je me suis pas\n",
      "++--------------++\n",
      "input seq  :  I got hot.\n",
      "decoded seq:  je me suis pas\n",
      "++--------------++\n",
      "input seq  :  I had fun.\n",
      "decoded seq:  je me suis pas\n",
      "++--------------++\n",
      "input seq  :  I had fun.\n",
      "decoded seq:  je me suis pas\n",
      "++--------------++\n",
      "input seq  :  I had fun.\n",
      "decoded seq:  je me suis pas\n",
      "++--------------++\n",
      "input seq  :  I had fun.\n",
      "decoded seq:  je me suis pas\n",
      "++--------------++\n",
      "input seq  :  I hate it.\n",
      "decoded seq:  je l'ai\n",
      "++--------------++\n",
      "input seq  :  I hope so.\n",
      "decoded seq:  j'ai besoin\n",
      "++--------------++\n",
      "input seq  :  I knew it.\n",
      "decoded seq:  je l'ai\n",
      "++--------------++\n",
      "input seq  :  I like it.\n",
      "decoded seq:  j'aime le\n",
      "++--------------++\n",
      "input seq  :  I lost it.\n",
      "decoded seq:  je l'ai\n",
      "++--------------++\n",
      "input seq  :  I love it!\n",
      "decoded seq:  j'aime\n",
      "++--------------++\n",
      "input seq  :  I love it.\n",
      "decoded seq:  j'aime\n",
      "++--------------++\n",
      "input seq  :  I mean it!\n",
      "decoded seq:  je l'ai\n",
      "++--------------++\n",
      "input seq  :  I mean it.\n",
      "decoded seq:  je l'ai\n",
      "++--------------++\n",
      "input seq  :  I must go.\n",
      "decoded seq:  je me pas\n",
      "++--------------++\n",
      "input seq  :  I must go.\n",
      "decoded seq:  je me pas\n",
      "++--------------++\n",
      "input seq  :  I must go.\n",
      "decoded seq:  je me pas\n",
      "++--------------++\n",
      "input seq  :  I must go.\n",
      "decoded seq:  je me pas\n",
      "++--------------++\n",
      "input seq  :  I must go.\n",
      "decoded seq:  je me pas\n",
      "++--------------++\n",
      "input seq  :  I must go.\n",
      "decoded seq:  je me pas\n",
      "++--------------++\n",
      "input seq  :  I must go.\n",
      "decoded seq:  je me pas\n",
      "++--------------++\n",
      "input seq  :  I must go.\n",
      "decoded seq:  je me pas\n",
      "++--------------++\n",
      "input seq  :  I need it.\n",
      "decoded seq:  j'ai le de\n",
      "++--------------++\n",
      "input seq  :  I need it.\n",
      "decoded seq:  j'ai le de\n",
      "++--------------++\n",
      "input seq  :  I noticed.\n",
      "decoded seq:  je été\n",
      "++--------------++\n",
      "input seq  :  I promise.\n",
      "decoded seq:  je l'ai\n",
      "++--------------++\n",
      "input seq  :  I relaxed.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I relaxed.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I said no.\n",
      "decoded seq:  ne soyez pas\n",
      "++--------------++\n",
      "input seq  :  I said so.\n",
      "decoded seq:  j'ai le de\n",
      "++--------------++\n",
      "input seq  :  I saw him.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I saw him.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I saw him.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I saw one.\n",
      "decoded seq:  je suis en\n",
      "++--------------++\n",
      "input seq  :  I saw one.\n",
      "decoded seq:  je suis en\n",
      "++--------------++\n",
      "input seq  :  I saw you.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I saw you.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I saw you.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I saw you.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I saw you.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I saw you.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I saw you.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I saw you.\n",
      "decoded seq:  je suis ai\n",
      "++--------------++\n",
      "input seq  :  I see Tom.\n",
      "decoded seq:  j'ai le de\n",
      "++--------------++\n",
      "input seq  :  I shouted.\n",
      "decoded seq:  j'ai été\n",
      "++--------------++\n",
      "input seq  :  I tripped.\n",
      "decoded seq:  j'ai été\n",
      "++--------------++\n",
      "input seq  :  I tripped.\n",
      "decoded seq:  j'ai été\n",
      "++--------------++\n",
      "input seq  :  I want it.\n",
      "decoded seq:  je l'ai\n",
      "++--------------++\n",
      "input seq  :  I was new.\n",
      "decoded seq:  j'aime\n",
      "++--------------++\n",
      "input seq  :  I was new.\n",
      "decoded seq:  j'aime\n",
      "++--------------++\n",
      "input seq  :  I will go.\n",
      "decoded seq:  j'ai le\n",
      "++--------------++\n",
      "input seq  :  I woke up.\n",
      "decoded seq:  je suis suis\n",
      "++--------------++\n",
      "input seq  :  I woke up.\n",
      "decoded seq:  je suis suis\n",
      "++--------------++\n",
      "input seq  :  I'd agree.\n",
      "decoded seq:  tu es\n",
      "++--------------++\n",
      "input seq  :  I'd leave.\n",
      "decoded seq:  laisse moi\n",
      "++--------------++\n",
      "input seq  :  I'll call.\n",
      "decoded seq:  je les\n",
      "++--------------++\n",
      "input seq  :  I'll cook.\n",
      "decoded seq:  je le\n",
      "++--------------++\n",
      "input seq  :  I'll help.\n",
      "decoded seq:  je le\n",
      "++--------------++\n",
      "input seq  :  I'll live.\n",
      "decoded seq:  je\n",
      "++--------------++\n",
      "input seq  :  I'll obey.\n",
      "decoded seq:  je\n",
      "++--------------++\n",
      "input seq  :  I'll pack.\n",
      "decoded seq:  je les un\n",
      "++--------------++\n",
      "input seq  :  I'll pack.\n",
      "decoded seq:  je les un\n",
      "++--------------++\n",
      "input seq  :  I'll pack.\n",
      "decoded seq:  je les un\n",
      "++--------------++\n",
      "input seq  :  I'll pass.\n",
      "decoded seq:  je faim\n",
      "++--------------++\n",
      "input seq  :  I'll quit.\n",
      "decoded seq:  je le\n",
      "++--------------++\n",
      "input seq  :  I'll sing.\n",
      "decoded seq:  je le\n",
      "++--------------++\n",
      "input seq  :  I'll stop.\n",
      "decoded seq:  je\n",
      "++--------------++\n",
      "input seq  :  I'll swim.\n",
      "decoded seq:  je le\n",
      "++--------------++\n",
      "input seq  :  I'll wait.\n",
      "decoded seq:  je\n",
      "++--------------++\n",
      "input seq  :  I'll walk.\n",
      "decoded seq:  je le\n",
      "++--------------++\n",
      "input seq  :  I'll work.\n",
      "decoded seq:  je le\n",
      "++--------------++\n",
      "input seq  :  I'll work.\n",
      "decoded seq:  je le\n",
      "++--------------++\n",
      "input seq  :  I'm a cop.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm a man.\n",
      "decoded seq:  je suis en\n",
      "++--------------++\n",
      "input seq  :  I'm alive.\n",
      "decoded seq:  je suis en\n",
      "++--------------++\n",
      "input seq  :  I'm alive.\n",
      "decoded seq:  je suis en\n",
      "++--------------++\n",
      "input seq  :  I'm alive.\n",
      "decoded seq:  je suis en\n",
      "++--------------++\n",
      "input seq  :  I'm alone.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm alone.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm armed.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm armed.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm awake.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm blind.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm broke.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm crazy.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm crazy.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm cured.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm cured.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm drunk.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm drunk.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm drunk.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm dying.\n",
      "decoded seq:  je suis suis pas\n",
      "++--------------++\n",
      "input seq  :  I'm early.\n",
      "decoded seq:  je suis\n",
      "++--------------++\n",
      "input seq  :  I'm first.\n",
      "decoded seq:  je suis ai\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "for seq_index in range(500, 600):\n",
    "    input_seq = preproc_input_texts[seq_index: seq_index+1]\n",
    "#     print('seq: ', input_seq)\n",
    "    # Reshape the input\n",
    "#     input_seq = input_seq.reshape((*input_seq.shape, 1))\n",
    "#     print(input_seq)\n",
    "    decoded_seq = model.predict(input_seq)\n",
    "    print('++--------------++')\n",
    "    print('input seq  : ', ' '.join(input_texts[seq_index: seq_index+1]))\n",
    "#     print('dseq: ', decoded_seq)\n",
    "    print('decoded seq: ', unpad(logits_to_text(decoded_seq[0], target_texts_tk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
